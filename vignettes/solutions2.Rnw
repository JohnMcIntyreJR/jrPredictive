%\VignetteIndexEntry{solutions2}
%\VignetteEngine{Sweave}


\documentclass[a4paper,justified,openany]{tufte-handout}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsmath}
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}
\title{Predictive Analytics: practical 2 solutions}
\date{} % if the \date{} command is left out, the current date will be used
\usepackage{booktabs}
\usepackage{units}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}
\newcommand{\cc}{\texttt}
\graphicspath{{../graphics/}}
\setcounter{secnumdepth}{2}
\usepackage{microtype}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{"caret"}\hlstd{)}
\hlkwd{data}\hlstd{(FuelEconomy,} \hlkwc{package} \hlstd{=} \hlstr{"AppliedPredictiveModeling"}\hlstd{)}
\hlkwd{set.seed}\hlstd{(}\hlnum{25}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\section*{Cross validation and the bootstrap}
\begin{itemize}
\item Fit a linear regression model to the \cc{cars2010} data set with \cc{FE} as the response, using \cc{EngDispl}, \cc{NumCyl} and \cc{NumGears} as predictors.\marginnote{The data set can be loaded \cc{data("FuelEconomy", package = "AppliedPredictiveModeling")}.}
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mLM} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,} \hlkwc{data} \hlstd{= cars2010)}
\end{alltt}
\end{kframe}
\end{knitrout}
\item What is the training error rate (RMSE) for this model?\marginnote{Hint: The training error can be found by taking the square root of the average square residuals. The \cc{sqrt} and \cc{resid} functions may be useful.}
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{res} \hlkwb{=} \hlkwd{resid}\hlstd{(mLM)}
\hlstd{(trainRMSE} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{mean}\hlstd{(res}\hlopt{*}\hlstd{res)))}
\end{alltt}
\begin{verbatim}
## [1] 4.59
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Re--train your model using the validation set approach to estimate a test RMSE, make your validation set equivalent to half of the entire data set.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## pick an index for samples}
\hlcom{## floor just rounds down so we only try to sample a}
\hlcom{## whole number}
\hlstd{index} \hlkwb{=} \hlkwd{sample}\hlstd{(}\hlkwd{nrow}\hlstd{(cars2010),}\hlkwd{floor}\hlstd{(}\hlkwd{nrow}\hlstd{(cars2010)}\hlopt{/}\hlnum{2}\hlstd{))}
\hlcom{## set up a train control object}
\hlstd{tcVS} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{index} \hlstd{=} \hlkwd{list}\hlstd{(}
    \hlkwc{Fold1} \hlstd{= (}\hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(cars2010))[}\hlopt{-}\hlstd{index]))}
\hlcom{## train the model}
\hlstd{mLMVS} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tcVS)}
\end{alltt}
\end{kframe}
\end{knitrout}
\item How does this compare to the training error that we estimated above?
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# it's larger, often training error under estimates test error}
\hlkwd{getTrainPerf}\hlstd{(mLMVS)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     4.566        0.6165    3.503     lm
\end{verbatim}
\begin{alltt}
\hlstd{trainRMSE}
\end{alltt}
\begin{verbatim}
## [1] 4.59
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Go through the same process using the different methods for estimating test error. That is leave one out and $k$--fold crossvalidation as well as bootstrapping.\marginnote{$10$--fold cross validation can be shown to be a good choice for almost any situation.}
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# set up train control objects}
\hlstd{tcLOOCV} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"LOOCV"}\hlstd{)}
\hlstd{tcKFOLD} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{)}
\hlstd{tcBOOT} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"boot"}\hlstd{)}

\hlcom{# train the model}
\hlstd{mLMLOOCV} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tcLOOCV)}
\hlstd{mLMKFOLD} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tcKFOLD)}
\hlstd{mLMBOOT} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tcBOOT)}
\end{alltt}
\end{kframe}
\end{knitrout}
\item How do these estimates compare with the validation set approach?
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mLMVS)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     4.566        0.6165    3.503     lm
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mLMLOOCV)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     4.612        0.6214    3.497     lm
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mLMKFOLD)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     4.596        0.6271     3.49     lm
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mLMBOOT)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     4.619        0.6244    3.513     lm
\end{verbatim}
\begin{alltt}
\hlcom{# all lower than validation set, we mentioned it tended to}
\hlcom{# over estimate test error}
\end{alltt}
\end{kframe}
\end{knitrout}
\item The object returned by \cc{train} also contains timing information that can be accessed via the \cc{times} component of the list.\marginnote{The \cc{\$} notation can be used pick a single list component.} Which of the methods is fastest?
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mLMVS}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   0.816   0.004   0.819
\end{verbatim}
\begin{alltt}
\hlstd{mLMLOOCV}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   6.784   0.004   6.789
\end{verbatim}
\begin{alltt}
\hlstd{mLMKFOLD}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   0.572   0.000   0.572
\end{verbatim}
\begin{alltt}
\hlstd{mLMBOOT}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
##   0.632   0.000   0.631
\end{verbatim}
\end{kframe}
\end{knitrout}
\item Using k--fold cross validation to estimate test error investigate how the number of folds effects the resultant estimates and computation time.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# a number of trainControl objects }
\hlstd{tc2} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{2}\hlstd{)}
\hlstd{tc5} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{5}\hlstd{)}
\hlstd{tc10} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{)}
\hlstd{tc15} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{15}\hlstd{)}
\hlstd{tc20} \hlkwb{=} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"cv"}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{20}\hlstd{)}
\hlcom{# train the model using each}
\hlstd{mLM2} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tc2)}
\hlstd{mLM5} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tc5)}
\hlstd{mLM10} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tc10)}
\hlstd{mLM15} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tc15)}
\hlstd{mLM20} \hlkwb{=} \hlkwd{train}\hlstd{(FE}\hlopt{~}\hlstd{EngDispl}\hlopt{+}\hlstd{NumCyl}\hlopt{+}\hlstd{NumGears,} \hlkwc{method} \hlstd{=} \hlstr{"lm"}\hlstd{,}
    \hlkwc{data} \hlstd{= cars2010,} \hlkwc{trControl} \hlstd{= tc20)}
\hlcom{# use a data frame to store all of the relevant information}
\hlstd{(info} \hlkwb{=} \hlkwd{data.frame}\hlstd{(}\hlstr{"Folds"} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,}\hlnum{5}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{15}\hlstd{,}\hlnum{20}\hlstd{),}
    \hlstr{"Time"} \hlstd{=} \hlkwd{c}\hlstd{(mLM2}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything[}\hlnum{1}\hlstd{],}
        \hlstd{mLM5}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything[}\hlnum{1}\hlstd{],}
        \hlstd{mLM10}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything[}\hlnum{1}\hlstd{],}
        \hlstd{mLM15}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything[}\hlnum{1}\hlstd{],}
        \hlstd{mLM20}\hlopt{$}\hlstd{times}\hlopt{$}\hlstd{everything[}\hlnum{1}\hlstd{]),}
    \hlstr{"Estimate"} \hlstd{=} \hlkwd{c}\hlstd{(mLM2}\hlopt{$}\hlstd{results}\hlopt{$}\hlstd{RMSE,}
                   \hlstd{mLM5}\hlopt{$}\hlstd{results}\hlopt{$}\hlstd{RMSE,}
                   \hlstd{mLM10}\hlopt{$}\hlstd{results}\hlopt{$}\hlstd{RMSE,}
                   \hlstd{mLM15}\hlopt{$}\hlstd{results}\hlopt{$}\hlstd{RMSE,}
                   \hlstd{mLM20}\hlopt{$}\hlstd{results}\hlopt{$}\hlstd{RMSE)))}
\end{alltt}
\begin{verbatim}
##   Folds  Time Estimate
## 1     2 0.464    4.620
## 2     5 0.436    4.614
## 3    10 0.476    4.600
## 4    15 0.504    4.579
## 5    20 0.508    4.565
\end{verbatim}
\begin{alltt}
\hlcom{# as there are more folds it takes longer to compute,}
\hlcom{# not an issue with such a small model but something}
\hlcom{# to consider on more complicated models.}
\hlcom{# Estimates are going down as the number of folds increases.}
\hlcom{# This is because for each held out fold we are using a greater}
\hlcom{# proportion of the data in training so expect to get a better}
\hlcom{# model.}
\end{alltt}
\end{kframe}
\end{knitrout}
\item Experiment with adding terms to the model, transformations of the predictors and interactions say and use cross validation to estimate test error for each. What is the best model you can find? You can still use the \cc{validate} and \cc{mark} functions to look at how your models fair on the unseen data. 
\end{itemize}

\section*{Penalised regression}

The \cc{diabetes} data set in the \cc{lars} package contains measurements of a number of predictors to model a response $y$, a measure of disease progression. There are other columns in the data set which contain interactions so we will extract just the predictors and the response. The data has already been normalized.

% <<>>=
% data(diabetes, package = "lars")
% 
% diabetesdata = cbind(diabetes$x,"y" = diabetes$y)
% # shortcut to create a model formula with all 2 way 
% # interactions and square terms.
% modelFormula = as.formula(paste("y~(.)^2 + ",
%   paste("I(",colnames(diabetesdata[,1:10]),"^2)", 
%         collapse = "+",sep = "")))
% 
% @

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## load the data in }
\hlkwd{data}\hlstd{(diabetes,} \hlkwc{package} \hlstd{=} \hlstr{"lars"}\hlstd{)}
\hlstd{diabetesdata} \hlkwb{=} \hlkwd{cbind}\hlstd{(diabetes}\hlopt{$}\hlstd{x,}\hlstr{"y"} \hlstd{= diabetes}\hlopt{$}\hlstd{y)}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{itemize}
\item Try fitting a lasso, ridge and elastic net model using all of the main effects, pairwise interactions and square terms from each of the predictors.\sidenote{Hint: see notes for shortcut on creating model formula. Also be aware that if the predictor is a factor a polynomial term doesn't make sense}
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{modelformula} \hlkwb{=} \hlkwd{as.formula}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{"y~(.)^2 + "}\hlstd{,}
    \hlkwd{paste0}\hlstd{(}\hlstr{"I("}\hlstd{,}
           \hlkwd{colnames}\hlstd{(diabetesdata),}\hlstr{"^2)"}\hlstd{,}
          \hlkwc{collapse} \hlstd{=} \hlstr{"+"}\hlstd{)}
    \hlstd{))}
\hlstd{mLASSO} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"lasso"}\hlstd{)}
\hlstd{mRIDGE} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"ridge"}\hlstd{)}
\hlstd{mENET} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"enet"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\marginnote{\cc{fraction = 0} is the same as the null model.}
\marginnote{ 
\cc{$y \sim (.) \wedge 2$}
is short hand for a model that includes pairwise interactions for each predictor, so if we use this we should only need to add the square terms}

% <<warning = FALSE, message = FALSE, echo = FALSE>>=
% m.lasso = train(modelFormula, data = diabetesdata, 
%         method = "lasso", 
%         tuneGrid = data.frame(fraction = seq(0,1,0.05)))
% @
% 
% <<warning = FALSE, message = FALSE, echo = FALSE>>=
% m.lasso = train(modelFormula, data = diabetesdata, 
%         method = "lasso", 
%         tuneGrid = data.frame(fraction = seq(0,1,0.05)))
% @


  \item Try to narrow in on the region of lowest RMSE for each model, don't forget about the \cc{tuneGrid} argument to the train function.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# examine previous output then train over a finer grid near }
\hlcom{# the better end}
\hlstd{mLASSOfine} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"lasso"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{fraction} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.1}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.05}\hlstd{)))}
\hlstd{mLASSOfine}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##   fraction  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD
## 1     0.10 17.56   0.9544 14.27  4.117   0.005526 3.8827
## 2     0.15 17.15   0.9527 13.75  1.236   0.005694 0.9178
## 3     0.20 17.18   0.9518 13.71  1.211   0.005866 0.7475
## 4     0.25 17.30   0.9511 13.79  1.200   0.005828 0.7347
## 5     0.30 17.39   0.9507 13.84  1.174   0.005738 0.7032
## 6     0.35 17.46   0.9503 13.90  1.146   0.005615 0.6811
## 7     0.40 17.52   0.9500 13.93  1.140   0.005588 0.6779
## 8     0.45 17.56   0.9497 13.96  1.135   0.005584 0.6712
## 9     0.50 17.60   0.9495 13.98  1.130   0.005569 0.6653
\end{verbatim}
\begin{alltt}
\hlcom{# best still right down at the 0.1 end}
\hlstd{mLASSOfiner} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"lasso"}\hlstd{,}
    \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{fraction} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.01}\hlstd{,}\hlnum{0.15}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.01}\hlstd{)))}
\hlstd{mLASSOfiner}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    fraction  RMSE Rsquared   MAE RMSESD RsquaredSD   MAESD
## 1      0.01 41.40   0.9545 35.70 14.729   0.003674 12.8283
## 2      0.02 23.69   0.9551 20.05 11.443   0.004728 10.1557
## 3      0.03 18.94   0.9544 15.63  5.860   0.004238  5.3517
## 4      0.04 17.40   0.9536 14.10  2.625   0.004400  2.4079
## 5      0.05 17.02   0.9526 13.66  1.256   0.004902  0.9444
## 6      0.06 17.13   0.9516 13.68  1.267   0.005210  0.9034
## 7      0.07 17.32   0.9505 13.77  1.304   0.005498  0.9264
## 8      0.08 17.47   0.9496 13.85  1.327   0.005571  0.9459
## 9      0.09 17.59   0.9489 13.92  1.383   0.005708  0.9735
## 10     0.10 17.67   0.9485 13.96  1.414   0.005716  0.9837
## 11     0.11 17.75   0.9480 14.00  1.456   0.005829  0.9942
## 12     0.12 17.80   0.9477 14.02  1.487   0.005930  0.9963
## 13     0.13 17.86   0.9474 14.05  1.529   0.006079  1.0044
## 14     0.14 17.91   0.9472 14.07  1.578   0.006277  1.0141
## 15     0.15 17.95   0.9469 14.10  1.611   0.006424  1.0228
\end{verbatim}
\begin{alltt}
\hlcom{# best is}
\hlstd{mLASSOfiner}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##   fraction
## 5     0.05
\end{verbatim}
\begin{alltt}
\hlstd{mRIDGEfine} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"ridge"}\hlstd{,}
    \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{0.1}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.01}\hlstd{)))}
\hlstd{mRIDGEfine}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    lambda  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD
## 1    0.00 18.73   0.9449 14.49 1.5741   0.008968 0.8175
## 2    0.01 17.27   0.9525 13.67 0.6901   0.004777 0.4367
## 3    0.02 17.18   0.9530 13.60 0.6670   0.004784 0.4229
## 4    0.03 17.19   0.9528 13.60 0.6729   0.004981 0.4298
## 5    0.04 17.27   0.9524 13.64 0.6978   0.005300 0.4583
## 6    0.05 17.40   0.9516 13.72 0.7346   0.005699 0.5039
## 7    0.06 17.57   0.9507 13.83 0.7784   0.006148 0.5561
## 8    0.07 17.76   0.9496 13.97 0.8256   0.006627 0.6014
## 9    0.08 17.99   0.9484 14.13 0.8744   0.007125 0.6465
## 10   0.09 18.23   0.9470 14.31 0.9233   0.007632 0.6914
## 11   0.10 18.49   0.9456 14.51 0.9717   0.008142 0.7385
\end{verbatim}
\begin{alltt}
\hlstd{mRIDGEfiner} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,}\hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"ridge"}\hlstd{,}
    \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.005}\hlstd{,}\hlnum{0.03}\hlstd{,}\hlkwc{by} \hlstd{=} \hlnum{0.001}\hlstd{)))}
\hlstd{mRIDGEfiner}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    lambda  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD
## 1   0.005 17.02   0.9523 13.54 1.0938   0.005258 0.7999
## 2   0.006 16.98   0.9526 13.51 1.0749   0.005164 0.7886
## 3   0.007 16.94   0.9528 13.48 1.0584   0.005083 0.7795
## 4   0.008 16.91   0.9529 13.46 1.0434   0.005012 0.7717
## 5   0.009 16.89   0.9530 13.44 1.0297   0.004950 0.7638
## 6   0.010 16.87   0.9531 13.42 1.0171   0.004894 0.7569
## 7   0.011 16.85   0.9532 13.40 1.0054   0.004844 0.7504
## 8   0.012 16.83   0.9533 13.39 0.9945   0.004801 0.7447
## 9   0.013 16.82   0.9534 13.38 0.9844   0.004763 0.7392
## 10  0.014 16.81   0.9534 13.37 0.9751   0.004730 0.7343
## 11  0.015 16.80   0.9534 13.35 0.9664   0.004702 0.7299
## 12  0.016 16.79   0.9535 13.35 0.9583   0.004679 0.7256
## 13  0.017 16.79   0.9535 13.34 0.9508   0.004659 0.7214
## 14  0.018 16.78   0.9535 13.33 0.9438   0.004644 0.7177
## 15  0.019 16.78   0.9535 13.32 0.9374   0.004633 0.7144
## 16  0.020 16.78   0.9535 13.32 0.9314   0.004625 0.7117
## 17  0.021 16.78   0.9535 13.31 0.9259   0.004621 0.7093
## 18  0.022 16.78   0.9535 13.31 0.9209   0.004620 0.7072
## 19  0.023 16.78   0.9535 13.30 0.9162   0.004622 0.7053
## 20  0.024 16.78   0.9535 13.30 0.9120   0.004627 0.7036
## 21  0.025 16.78   0.9535 13.30 0.9081   0.004635 0.7021
## 22  0.026 16.79   0.9534 13.30 0.9046   0.004645 0.7011
## 23  0.027 16.79   0.9534 13.29 0.9014   0.004658 0.7003
## 24  0.028 16.80   0.9534 13.29 0.8985   0.004673 0.6994
## 25  0.029 16.80   0.9533 13.29 0.8960   0.004690 0.6982
## 26  0.030 16.81   0.9533 13.29 0.8937   0.004709 0.6976
\end{verbatim}
\begin{alltt}
\hlcom{# the best one}
\hlstd{mRIDGEfiner}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##    lambda
## 17  0.021
\end{verbatim}
\begin{alltt}
\hlstd{mENETfine} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"enet"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{expand.grid}\hlstd{(}
                         \hlkwc{lambda} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.001}\hlstd{,}\hlnum{0.01}\hlstd{,}\hlnum{0.1}\hlstd{),}
                         \hlkwc{fraction} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.4}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlnum{0.6}\hlstd{)}
    \hlstd{))}
\hlstd{mENETfine}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##   lambda fraction  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD
## 1  0.001      0.4 16.42   0.9557 13.17 0.7871   0.004794 0.6037
## 4  0.010      0.4 16.31   0.9582 13.57 0.9192   0.004445 0.8533
## 7  0.100      0.4 22.20   0.9558 19.24 2.0372   0.004023 1.8105
## 2  0.001      0.5 16.94   0.9528 13.47 1.0903   0.006248 0.7622
## 5  0.010      0.5 15.92   0.9584 12.93 0.6037   0.003734 0.4892
## 8  0.100      0.5 17.05   0.9557 14.03 0.8451   0.004205 0.7932
## 3  0.001      0.6 17.23   0.9512 13.67 1.2796   0.007268 0.8699
## 6  0.010      0.6 16.37   0.9559 13.09 0.7104   0.004202 0.5720
## 9  0.100      0.6 16.97   0.9526 13.50 0.6850   0.005499 0.5167
\end{verbatim}
\begin{alltt}
\hlstd{mENETfiner} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,}
    \hlkwc{method} \hlstd{=} \hlstr{"enet"}\hlstd{,} \hlkwc{tuneGrid} \hlstd{=} \hlkwd{expand.grid}\hlstd{(}
                         \hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.001}\hlstd{,}\hlnum{0.1}\hlstd{,}\hlkwc{length.out} \hlstd{=} \hlnum{10}\hlstd{),}
                         \hlkwc{fraction} \hlstd{=} \hlnum{0.5}\hlstd{))}
\hlstd{mENETfiner}\hlopt{$}\hlstd{results}
\end{alltt}
\begin{verbatim}
##    lambda fraction  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD
## 1   0.001      0.5 17.11   0.9509 13.67 0.9736   0.005624 0.6715
## 2   0.012      0.5 16.12   0.9568 13.15 0.7688   0.003916 0.6287
## 3   0.023      0.5 16.25   0.9567 13.38 0.7307   0.003836 0.6390
## 4   0.034      0.5 16.52   0.9562 13.66 0.7750   0.003861 0.7486
## 5   0.045      0.5 16.79   0.9556 13.90 0.8293   0.003836 0.8282
## 6   0.056      0.5 17.02   0.9552 14.11 0.8777   0.003855 0.8891
## 7   0.067      0.5 17.20   0.9549 14.26 0.8935   0.003882 0.9052
## 8   0.078      0.5 17.33   0.9546 14.37 0.8929   0.003970 0.8969
## 9   0.089      0.5 17.41   0.9544 14.44 0.8872   0.004072 0.8788
## 10  0.100      0.5 17.47   0.9542 14.47 0.8819   0.004187 0.8589
\end{verbatim}
\begin{alltt}
\hlstd{mENETfiner}\hlopt{$}\hlstd{bestTune}
\end{alltt}
\begin{verbatim}
##   fraction lambda
## 2      0.5  0.012
\end{verbatim}
\end{kframe}
\end{knitrout}
\noindent We can view the coefficients via
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{coef} \hlkwb{=} \hlkwd{predict}\hlstd{(mLASSO}\hlopt{$}\hlstd{finalModel,}
  \hlkwc{mode} \hlstd{=} \hlstr{"fraction"}\hlstd{,}
  \hlkwc{s} \hlstd{= mLASSO}\hlopt{$}\hlstd{bestTune}\hlopt{$}\hlstd{fraction,}\hlcom{# which ever fraction was chosen as best}
  \hlkwc{type} \hlstd{=} \hlstr{"coefficients"}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


  \item How many features have been chosen by the \cc{lasso} and \cc{enet} models?
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# use predict to find the coefficients}
\hlstd{coefLASSO} \hlkwb{=} \hlkwd{predict}\hlstd{(mLASSOfiner}\hlopt{$}\hlstd{finalModel,} \hlkwc{mode} \hlstd{=} \hlstr{"fraction"}\hlstd{,}
        \hlkwc{type} \hlstd{=} \hlstr{"coefficient"}\hlstd{,} \hlkwc{s} \hlstd{= mLASSO}\hlopt{$}\hlstd{bestTune}\hlopt{$}\hlstd{fraction,}
        \hlstd{)}
\hlkwd{sum}\hlstd{(coefLASSO}\hlopt{$}\hlstd{coefficients} \hlopt{!=} \hlnum{0}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 57
\end{verbatim}
\begin{alltt}
\hlstd{coefENET}\hlkwb{=} \hlkwd{predict}\hlstd{(mENETfiner}\hlopt{$}\hlstd{finalModel,} \hlkwc{mode} \hlstd{=} \hlstr{"fraction"}\hlstd{,}
        \hlkwc{type} \hlstd{=} \hlstr{"coefficient"}\hlstd{,} \hlkwc{s} \hlstd{= mENET}\hlopt{$}\hlstd{bestTune}\hlopt{$}\hlstd{fraction}
        \hlstd{)}
\hlkwd{sum}\hlstd{(coefENET}\hlopt{$}\hlstd{coefficients} \hlopt{!=} \hlnum{0}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 24
\end{verbatim}
\end{kframe}
\end{knitrout}
  \item How do these models compare to principal components and partial least squares regression?
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mPCR} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,} \hlkwc{method} \hlstd{=} \hlstr{"pcr"}\hlstd{,}
             \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{7}\hlstd{))}
\hlstd{mPLS} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,} \hlkwc{method} \hlstd{=} \hlstr{"pls"}\hlstd{,}
             \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp}\hlstd{=} \hlnum{1}\hlopt{:}\hlnum{7}\hlstd{))}
\hlstd{mPLS2} \hlkwb{=} \hlkwd{train}\hlstd{(modelformula,} \hlkwc{data} \hlstd{= diabetesdata,} \hlkwc{method} \hlstd{=} \hlstr{"pls"}\hlstd{,}
             \hlkwc{tuneGrid} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{ncomp}\hlstd{=} \hlnum{5}\hlopt{:}\hlnum{15}\hlstd{))}
\hlkwd{getTrainPerf}\hlstd{(mLASSOfiner)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     17.02        0.9526    13.66  lasso
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mRIDGEfiner)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     16.78        0.9535    13.31  ridge
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mENETfiner)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     16.12        0.9568    13.15   enet
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mPCR)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     16.18         0.956    13.29    pcr
\end{verbatim}
\begin{alltt}
\hlkwd{getTrainPerf}\hlstd{(mPLS2)}
\end{alltt}
\begin{verbatim}
##   TrainRMSE TrainRsquared TrainMAE method
## 1     15.83        0.9595    12.85    pls
\end{verbatim}
\begin{alltt}
\hlcom{#The elastic net model has the lowest estimated test error, all are fairly similar. The elastic net model suggests only 21 non--zero}
\hlcom{#coefficients out of all of those included in the model.}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{itemize}

\end{document}

